{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10154354",
   "metadata": {},
   "source": [
    "## Uploading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c7b89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c462a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes = []\n",
    "for line in open('Entity Recognition in Resumes.json', 'r'):\n",
    "    resumes.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d644b0",
   "metadata": {},
   "source": [
    "## Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "641b151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = list()\n",
    "for line in resumes:\n",
    "    res_text = line['content'].replace(\"\\n\", \" \")\n",
    "    res_ent = list()\n",
    "    annoted_info = line[\"annotation\"]\n",
    "\n",
    "    if annoted_info is not None:\n",
    "#         res_labels = list()\n",
    "        for anno in annoted_info:\n",
    "            point = anno['points'][0]\n",
    "            res_labels = anno['label']\n",
    "            if not isinstance(res_labels, list):\n",
    "                res_labels = [res_labels]\n",
    "        \n",
    "            for label in res_labels:\n",
    "                point_start = point['start']\n",
    "                point_end = point['end']\n",
    "                point_text = point['text']\n",
    "        \n",
    "        \n",
    "                lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                if lstrip_diff != 0:\n",
    "                    point_start = point_start + lstrip_diff\n",
    "                if rstrip_diff != 0:\n",
    "                    point_end = point_end - rstrip_diff\n",
    "                res_ent.append((point_start, point_end + 1 , label))\n",
    "    training_data.append((res_text, {\"entities\" : res_ent}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb46213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  • To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK EXPERIENCE  Application Development Associate  Accenture -  November 2017 to Present  Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input. Also, Training the bot for different possible utterances (Both positive and negative), which will be given as input by the user.  EDUCATION  B.E in Information science and engineering  B.v.b college of engineering and technology -  Hubli, Karnataka  August 2013 to June 2017  12th in Mathematics  Woodbine modern school  April 2011 to March 2013  10th  Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)  ADDITIONAL INFORMATION  Technical Skills  https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   • Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of Things • Machine Learning • Database Management System • Computer Networks • Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • Honest and Hard-Working • Tolerant and Flexible to Different Situations • Polite and Calm • Team-Player\",\n",
       " {'entities': [(1296, 1622, 'Skills'),\n",
       "   (993, 1154, 'Skills'),\n",
       "   (939, 957, 'College Name'),\n",
       "   (883, 905, 'College Name'),\n",
       "   (856, 860, 'Graduation Year'),\n",
       "   (771, 814, 'College Name'),\n",
       "   (727, 769, 'Designation'),\n",
       "   (407, 416, 'Companies worked at'),\n",
       "   (372, 405, 'Designation'),\n",
       "   (95, 145, 'Email Address'),\n",
       "   (60, 69, 'Location'),\n",
       "   (49, 58, 'Companies worked at'),\n",
       "   (13, 46, 'Designation'),\n",
       "   (0, 12, 'Name')]})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5112f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "260b24fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = trim_entity_spans(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d24857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Afreen Jamadar Active member of IIIT Committee in Third year  Sangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-Jamadar/8baf379b705e37c6  I wish to use my knowledge, skills and conceptual understanding to create excellent team environments and work consistently achieving organization objectives believes in taking initiative and work to excellence in my work.  WORK EXPERIENCE  Active member of IIIT Committee in Third year  Cisco Networking -  Kanpur, Uttar Pradesh  organized by Techkriti IIT Kanpur and Azure Skynet. PERSONALLITY TRAITS: • Quick learning ability • hard working  EDUCATION  PG-DAC  CDAC ACTS  2017  Bachelor of Engg in Information Technology  Shivaji University Kolhapur -  Kolhapur, Maharashtra  2016  SKILLS  Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT ACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)  ADDITIONAL INFORMATION  TECHNICAL SKILLS:  • Programming Languages: C, C++, Java, .net, php. • Web Designing: HTML, XML • Operating Systems: Windows […] Windows Server 2003, Linux. • Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.  https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN',\n",
       " {'entities': [[1155, 1199, 'Email Address'],\n",
       "   [743, 1141, 'Skills'],\n",
       "   [729, 733, 'Graduation Year'],\n",
       "   [675, 702, 'College Name'],\n",
       "   [631, 673, 'Degree'],\n",
       "   [625, 629, 'Graduation Year'],\n",
       "   [614, 623, 'College Name'],\n",
       "   [606, 612, 'Degree'],\n",
       "   [104, 148, 'Email Address'],\n",
       "   [62, 68, 'Location'],\n",
       "   [0, 14, 'Name']]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7466aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entities(training_data):\n",
    "    \n",
    "    clean_data = []\n",
    "    for text, annotation in training_data:\n",
    "       \n",
    "        entities = annotation.get('entities')\n",
    "        entities_copy = entities.copy()\n",
    "       \n",
    "         # append entity only if it is longer than its overlapping entity\n",
    "        i = 0\n",
    "        for entity in entities_copy:\n",
    "            j = 0\n",
    "            for overlapping_entity in entities_copy:\n",
    "                 # Skip self\n",
    "                if i != j:\n",
    "                    e_start, e_end, oe_start, oe_end = entity[0], entity[1], overlapping_entity[0], overlapping_entity[1]\n",
    "                    # Delete any entity that overlaps, keep if longer\n",
    "                    if ((e_start >= oe_start and e_start <= oe_end) \\\n",
    "                    or (e_end <= oe_end and e_end >= oe_start)) \\\n",
    "                    and ((e_end - e_start) <= (oe_end - oe_start)):\n",
    "                        entities.remove(entity)\n",
    "                j += 1\n",
    "            i += 1\n",
    "        clean_data.append((text, {'entities': entities}))\n",
    "                \n",
    "    return clean_data\n",
    "\n",
    "extra_cleaned_data = clean_entities(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcefbbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy==2.1.4 in /home/iki/.local/lib/python3.8/site-packages (2.1.4)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /home/iki/.local/lib/python3.8/site-packages (from spacy==2.1.4) (0.2.4)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /home/iki/.local/lib/python3.8/site-packages (from spacy==2.1.4) (2.0.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /home/iki/.local/lib/python3.8/site-packages (from spacy==2.1.4) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/iki/.local/lib/python3.8/site-packages (from spacy==2.1.4) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/iki/.local/lib/python3.8/site-packages (from spacy==2.1.4) (1.0.8)\n",
      "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /home/iki/.local/lib/python3.8/site-packages (from spacy==2.1.4) (3.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /home/iki/.local/lib/python3.8/site-packages (from spacy==2.1.4) (0.10.1)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /home/iki/.local/lib/python3.8/site-packages (from spacy==2.1.4) (7.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy==2.1.4) (2.22.0)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/iki/.local/lib/python3.8/site-packages (from spacy==2.1.4) (0.9.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/iki/.local/lib/python3.8/site-packages (from spacy==2.1.4) (1.22.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/iki/.local/lib/python3.8/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (21.2.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/iki/.local/lib/python3.8/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (45.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/iki/.local/lib/python3.8/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (0.18.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/iki/.local/lib/python3.8/site-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.4) (4.62.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==2.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07ecab",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e1ce88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def train_test_split(data, test_size, random_state):\n",
    "\n",
    "    random.Random(random_state).shuffle(data)\n",
    "    test_idx = len(data) - math.floor(test_size * len(data))\n",
    "    train_set = data[0: test_idx]\n",
    "    test_set = data[test_idx: ]\n",
    "\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28de9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(extra_cleaned_data, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4967c2",
   "metadata": {},
   "source": [
    "### Training SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fab78a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f340e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spacy():\n",
    "    nlp = spacy.blank('en')\n",
    "    \n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        res_entity_recog = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(res_entity_recog, last=True)\n",
    "        \n",
    "    for _, anno in train_data:\n",
    "        for ent in anno.get(\"entities\"):\n",
    "            res_entity_recog.add_label(ent[2])\n",
    "            \n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(10):\n",
    "            print(\"Statring iteration \" + str(itn))\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            for text, annotations in train_data:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4c48350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statring iteration 0\n",
      "{'ner': 21215.14240076845}\n",
      "Statring iteration 1\n",
      "{'ner': 16183.839795593864}\n",
      "Statring iteration 2\n",
      "{'ner': 15613.23278257941}\n",
      "Statring iteration 3\n",
      "{'ner': 10642.00645072839}\n",
      "Statring iteration 4\n",
      "{'ner': 10658.365409684346}\n",
      "Statring iteration 5\n",
      "{'ner': 10082.36266356942}\n",
      "Statring iteration 6\n",
      "{'ner': 9725.39428014704}\n",
      "Statring iteration 7\n",
      "{'ner': 9027.464786986107}\n",
      "Statring iteration 8\n",
      "{'ner': 9726.971812602676}\n",
      "Statring iteration 9\n",
      "{'ner': 9218.05092095899}\n"
     ]
    }
   ],
   "source": [
    "nlp = train_spacy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c1d4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.gold import GoldParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6261f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25b8f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_bilou(nlp, text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [(tok.text, tok.idx, tok.ent_type_) for tok in doc]\n",
    "    entities = []\n",
    "    for entity, group in groupby(tokens, key=lambda t: t[-1]):\n",
    "        if not entity:\n",
    "            continue\n",
    "        group = list(group)\n",
    "        _, start, _ = group[0]\n",
    "        word, last, _ = group[-1]\n",
    "        end = last + len(word)\n",
    "        entities.append((\n",
    "                    start,\n",
    "                    end,\n",
    "                    entity\n",
    "                ))\n",
    "    gold = GoldParse(nlp(text), entities = entities)\n",
    "    pred_ents = gold.ner\n",
    "    \n",
    "    return pred_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9048003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test = []\n",
    "y_pred = []\n",
    "\n",
    "for text, annots in test_data:\n",
    "    \n",
    "    gold = GoldParse(nlp.make_doc(text), entities = annots.get(\"entities\"))\n",
    "    ents = gold.ner\n",
    "    pred_ents = doc_to_bilou(nlp, text)\n",
    "    \n",
    "    y_test.append(ents)\n",
    "    y_pred.append(pred_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415f4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34fa791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "277cbaaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "                    -       0.00      0.00      0.00       428\n",
      "       B-College Name       0.63      0.78      0.69        54\n",
      "       I-College Name       0.65      0.75      0.69       103\n",
      "       L-College Name       0.57      0.70      0.63        54\n",
      "       U-College Name       1.00      0.50      0.67         6\n",
      "B-Companies worked at       0.51      0.52      0.52        48\n",
      "I-Companies worked at       0.01      0.30      0.03        10\n",
      "L-Companies worked at       0.47      0.48      0.47        48\n",
      "U-Companies worked at       0.43      0.36      0.39        92\n",
      "             B-Degree       0.90      0.84      0.87        45\n",
      "             I-Degree       0.83      0.93      0.88       132\n",
      "             L-Degree       0.88      0.82      0.85        45\n",
      "             U-Degree       0.38      0.60      0.46         5\n",
      "        B-Designation       0.53      0.79      0.64        91\n",
      "        I-Designation       0.42      0.61      0.50       116\n",
      "        L-Designation       0.49      0.73      0.58        91\n",
      "        U-Designation       0.00      0.00      0.00         1\n",
      "      B-Email Address       0.83      0.94      0.88        16\n",
      "      I-Email Address       0.92      0.96      0.94        50\n",
      "      L-Email Address       0.89      1.00      0.94        16\n",
      "      U-Email Address       0.78      1.00      0.88        21\n",
      "    U-Graduation Year       0.31      0.58      0.40        57\n",
      "           B-Location       1.00      0.40      0.57         5\n",
      "           L-Location       1.00      0.40      0.57         5\n",
      "           U-Location       0.63      0.63      0.63        68\n",
      "               B-Name       0.98      0.93      0.96        46\n",
      "               I-Name       0.00      0.00      0.00         2\n",
      "               L-Name       0.93      0.89      0.91        46\n",
      "                    O       0.95      0.96      0.96     27131\n",
      "             B-Skills       0.68      0.46      0.55        50\n",
      "             I-Skills       0.67      0.52      0.58      1693\n",
      "             L-Skills       0.62      0.42      0.50        50\n",
      "             U-Skills       0.00      0.00      0.00         3\n",
      "B-Years of Experience       0.33      0.11      0.17         9\n",
      "I-Years of Experience       0.00      0.00      0.00         2\n",
      "L-Years of Experience       0.33      0.11      0.17         9\n",
      "U-Years of Experience       0.00      0.00      0.00         1\n",
      "\n",
      "            micro avg       0.91      0.91      0.91     30649\n",
      "            macro avg       0.56      0.54      0.53     30649\n",
      "         weighted avg       0.90      0.91      0.91     30649\n",
      "          samples avg       0.91      0.91      0.91     30649\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iki/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/iki/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def ner_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_)\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset\n",
    "    ), accuracy_score(y_true_combined, y_pred_combined)\n",
    "    \n",
    "report, accuracy = ner_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65903e",
   "metadata": {},
   "source": [
    "## Testing the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f2e2067",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('new_custom_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc706973",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('new_custom_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53f07587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "test_doc = training_data[random.randint(0,200)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4aaed9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alok Khandai 0 12 Name\n",
      "\n",
      "\n",
      "Operational Analyst (SQL DBA) Engineer 13 51 Designation\n",
      "\n",
      "\n",
      "UNISYS 54 60 Companies worked at\n",
      "\n",
      "\n",
      "Bengaluru 62 71 Location\n",
      "\n",
      "\n",
      "indeed.com/r/Alok-Khandai/5be849e443b8f467 105 147 Email Address\n",
      "\n",
      "\n",
      "3.5 Years 158 167 Years of Experience\n",
      "\n",
      "\n",
      "Operational Analyst (SQL DBA) Engineer 1472 1510 Designation\n",
      "\n",
      "\n",
      "UNISYS 1512 1518 Companies worked at\n",
      "\n",
      "\n",
      "Bengaluru 1522 1531 Location\n",
      "\n",
      "\n",
      "2016 1551 1555 Graduation Year\n",
      "\n",
      "\n",
      "Microsoft Corporation 2339 2360 Companies worked at\n",
      "\n",
      "\n",
      "Microsoft 2809 2818 Companies worked at\n",
      "\n",
      "\n",
      "Microsoft 4079 4088 Companies worked at\n",
      "\n",
      "\n",
      "Microsoft Corporation 6199 6220 Companies worked at\n",
      "\n",
      "\n",
      "B.Tech in Computer Science and Engineering in CSE 7904 7953 Degree\n",
      "\n",
      "\n",
      "Indira Gandhi Institute Of Technology 7955 7992 College Name\n",
      "\n",
      "\n",
      "2012 7994 7998 Graduation Year\n",
      "\n",
      "\n",
      "Database (3 years), SQL (3 years), Sql Dba 8008 8050 Skills\n",
      "\n",
      "\n",
      "C, C++, PL/SQL 8370 8384 Skills\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_document = model(\" \".join(test_doc.split('\\n')))\n",
    "for ent in test_document.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54835643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('UNKNOWN',\n",
       " 'Companies worked at',\n",
       " 'Degree',\n",
       " 'College Name',\n",
       " 'Skills',\n",
       " 'Email Address',\n",
       " 'Location',\n",
       " 'Years of Experience',\n",
       " 'Designation',\n",
       " 'Name',\n",
       " 'Graduation Year')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.entity.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e0e41b",
   "metadata": {},
   "source": [
    "## As you can see based on the skills, companies worked at and designation one can sort the resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b2c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
